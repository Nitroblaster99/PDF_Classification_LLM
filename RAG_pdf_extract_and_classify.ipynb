{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKyB2wBpz5x7cBEptPtWXo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nitroblaster99/YannisGerontopoulos_MLE_Assignment/blob/main/RAG_pdf_extract_and_classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Libraries"
      ],
      "metadata": {
        "id": "ANsPR6IiiCPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install langchain-community --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install pypdf --quiet\n",
        "!pip install faiss-cpu --quiet\n",
        "!pip install flash-attention --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install langchain-huggingface --quiet"
      ],
      "metadata": {
        "id": "v-TJXvkNBWnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPkGrXepA1iB"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Ignore warnings to keep the output clean\n",
        "from langchain.document_loaders import PyPDFLoader  # To load PDFs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # To split large documents into chunks\n",
        "from langchain_core.prompts import PromptTemplate  # To define custom prompts for the language model\n",
        "from langchain.chains import RetrievalQA  # For performing Question-Answer retrieval\n",
        "import torch\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Embeddings for FAISS\n",
        "from langchain import HuggingFaceHub  # HuggingFace model loading\n",
        "from langchain.vectorstores import FAISS  # FAISS for document indexing and retrieval\n",
        "import re  # Regex for cleaning text and extracting relevant info\n",
        "import zipfile  # For extracting uploaded zip file\n",
        "import json  # For saving final output in JSON format\n",
        "from google.colab import files  # For file upload/download in Google Colab\n",
        "import os  # To work with file paths"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload Zipped Files"
      ],
      "metadata": {
        "id": "Rn3u5AX8iAbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the zip file containing PDFs\n",
        "uploaded = files.upload()  # This triggers file upload via Google Colab UI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UeVYghPaDVn6",
        "outputId": "f4202bd0-a84e-4c47-ea91-f61d6745e0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f2904124-a707-4cde-8ceb-93069cc4878d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f2904124-a707-4cde-8ceb-93069cc4878d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ICDAR2024_papers.zip to ICDAR2024_papers.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the uploaded zip file\n",
        "zip_filename = list(uploaded.keys())[0]  # Get the uploaded zip file name\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/ICDAR2024_papers/ICDAR2024_papers.zip')  # Extract the contents of the zip file into a folder"
      ],
      "metadata": {
        "id": "nv6OgQ-Qg7jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get all PDF file paths from the extracted folder\n",
        "def get_pdf_paths_from_folder(folder_path):\n",
        "    \"\"\"Walk through the folder and collect all PDF file paths.\"\"\"\n",
        "    pdf_paths = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):  # Only collect PDF files\n",
        "                pdf_paths.append(os.path.join(root, file))\n",
        "    return pdf_paths"
      ],
      "metadata": {
        "id": "uQ3nY_pLfKca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to open and clean PDFs\n",
        "def open_pdfs(file_path):\n",
        "    \"\"\"Load and clean the text from the PDF.\"\"\"\n",
        "    try:\n",
        "        pdf_loader = PyPDFLoader(file_path)  # Use PyPDFLoader to load the PDF\n",
        "        pdf = pdf_loader.load()  # Load PDF content\n",
        "\n",
        "        # Clean up the text content by removing excessive newlines\n",
        "        cleaned_pdf = []\n",
        "        if isinstance(pdf, list):  # If multiple documents are returned (edge case)\n",
        "            for doc in pdf:\n",
        "                doc.page_content = \" \".join(doc.page_content.split())  # Remove excessive newlines\n",
        "                cleaned_pdf.append(doc)\n",
        "            return cleaned_pdf\n",
        "        else:\n",
        "            pdf.page_content = \" \".join(pdf.page_content.split())  # Clean single document\n",
        "            return [pdf]\n",
        "    except Exception as e:\n",
        "        return [f\"Error opening {file_path}: {e}\"]  # Handle errors"
      ],
      "metadata": {
        "id": "09vBwtgOfzQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Documents"
      ],
      "metadata": {
        "id": "3mqKxBxmiQ7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the documetns\n",
        "def text_splits(data, size: int, overlap: int):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "NynL9MITA3sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Embedding and LLM model"
      ],
      "metadata": {
        "id": "5RCL2PsBiVUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name='mixedbread-ai/mxbai-embed-large-v1')  # Load embedding model once"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GelA_p7bgZGX",
        "outputId": "72dd2c89-a385-49b4-dfeb-713abce77be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f31d61ab66f5>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name='mixedbread-ai/mxbai-embed-large-v1')  # Load embedding model once\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LLM\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",  # Specify the LLM\n",
        "    model_kwargs=dict(max_new_tokens=1024, temperature=0.1, verbose=False),  # LLM parameters\n",
        "    huggingfacehub_api_token=\"\"  # HuggingFace API token\n",
        ")"
      ],
      "metadata": {
        "id": "YupE8wNlgZDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf45e9e-a994-4ed3-ed80-5cc5bde95628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-eeb2260f1cee>:2: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceHub(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize json format and helper functions"
      ],
      "metadata": {
        "id": "e23xvjHnidXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the JSON structure\n",
        "final_json = {\n",
        "    \"tables\": [],\n",
        "    \"classification\": [],\n",
        "    \"keyInformationExtraction\": [],\n",
        "    \"opticalCharacterRecognition\": [],\n",
        "    \"datasets\": [],\n",
        "    \"layoutUnderstanding\": [],\n",
        "    \"others\": []\n",
        "}"
      ],
      "metadata": {
        "id": "daGxw1xzAggf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to normalize strings (lowercase, no punctuation, no extra spaces)\n",
        "def normalize_string(s):\n",
        "    \"\"\"Normalize a string by lowercasing, removing punctuation, and extra whitespace.\"\"\"\n",
        "    return ' '.join(re.sub(r'[^\\w\\s]', '', s.lower()).split())\n",
        "\n",
        "# Normalize the list of authors (remove brackets and extra spaces)\n",
        "def normalize_authors(authors):\n",
        "    \"\"\"Normalize the authors list by removing brackets and extra whitespace.\"\"\"\n",
        "    return [normalize_string(author.strip('[]')) for author in authors]\n",
        "\n",
        "# Check if an entry already exists in the category list to avoid duplicates\n",
        "def entry_exists(category_list, title, authors):\n",
        "    \"\"\"Check if an entry with the same normalized title and authors exists.\"\"\"\n",
        "    normalized_title = normalize_string(title)\n",
        "    normalized_authors = normalize_authors(authors)\n",
        "    for entry in category_list:\n",
        "        if (normalize_string(entry[\"title\"]) == normalized_title and\n",
        "            normalize_authors(entry[\"authors\"]) == normalized_authors):\n",
        "            return True  # Entry exists\n",
        "    return False  # Entry doesn't exist"
      ],
      "metadata": {
        "id": "VvXOSxELYx7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the paths of all PDFs from the extracted folder\n",
        "folder_path = '/content/ICDAR2024_papers/ICDAR2024_papers.zip/ICDAR2024_proceedings_pdfs'  # Path to extracted folder\n",
        "file_paths = get_pdf_paths_from_folder(folder_path)"
      ],
      "metadata": {
        "id": "7eaNtfcLd6OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Retriever, Prompt Template and run RetrivalQA chain"
      ],
      "metadata": {
        "id": "SdhSpZ4bizzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a set to keep track of processed entries to avoid duplicates\n",
        "processed_entries = set()\n",
        "\n",
        "# Process each PDF file\n",
        "for file_path in file_paths:\n",
        "    # Load and clean the PDF\n",
        "    opened_pdfs = open_pdfs(file_path)\n",
        "    splits = text_splits(opened_pdfs, 2048, 512)  # Split the PDF into chunks\n",
        "\n",
        "    # Create a vector store and retriever for the current document\n",
        "    vector_store = FAISS.from_documents(splits, embeddings)  # Create FAISS index for this document\n",
        "    retriever = vector_store.as_retriever()  # Create a retriever from the FAISS index\n",
        "\n",
        "    # QA chain setup for this document\n",
        "    qa_stuff = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",  # Using the \"stuff\" chain type for simple Q&A\n",
        "        retriever=retriever,\n",
        "        verbose=False,\n",
        "        return_source_documents=False,\n",
        "    )\n",
        "\n",
        "    # Define the prompt for extracting metadata from the paper\n",
        "    prompt_template = \"\"\"Your task is to read the content of a scientific paper.\n",
        "    First, extract the title of the paper which is usually on the top.\n",
        "    After, list all the authors from the paper.\n",
        "    Then, based on the content and focus of the paper, categorize it into one of the following categories: Tables, Classification,\n",
        "    Key Information Extraction, Optical Character Recognition (OCR), Datasets, Document Layout Understanding, or Others.\n",
        "\n",
        "    After processing the paper, return only the following in a structured format:\n",
        "    Title: The title of the paper.\n",
        "    Authors: A list of authors.\n",
        "    Category: One of the above categories based on the paper's primary subject matter.\n",
        "    Ensure your categorization is based on the main focus of the paper, even if it overlaps with multiple categories.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the response from the model\n",
        "    response = qa_stuff.run(prompt_template)\n",
        "\n",
        "    # Clean the response\n",
        "    cleaned_response = re.sub(r'Use the following pieces of context to answer the question.*?Helpful Answer:', '', response, flags=re.DOTALL).strip()\n",
        "    final_response = cleaned_response.replace('\\n\\n', ' ')  # Replace double newlines with a space\n",
        "\n",
        "    # Extract the title, authors, and category using regex patterns\n",
        "\n",
        "    # Extract the title by searching for \"Title:\" followed by any characters (.*)\n",
        "    title_match = re.search(r\"Title:\\s*(.*)\", final_response)\n",
        "    # Extract the authors by searching for \"Authors:\" followed by any characters (.*)\n",
        "    authors_match = re.search(r\"Authors:\\s*(.*)\", final_response)\n",
        "    # Extract the category by searching for \"Category:\" followed by any characters (.*)\n",
        "    category_match = re.search(r\"Category:\\s*(.*)\", final_response)\n",
        "\n",
        "    # Get the file name from the file path\n",
        "    file_name = os.path.basename(file_path)\n",
        "    # If a title match is found, strip extra spaces; otherwise, return \"Title not found\"\n",
        "    title = title_match.group(1).strip() if title_match else \"Title not found\"\n",
        "    # If authors are found, split them by comma and space, otherwise return an empty list\n",
        "    authors = authors_match.group(1).strip().split(\", \") if authors_match else []\n",
        "    # If a category match is found, strip extra spaces; otherwise, return \"Other\"\n",
        "    category = category_match.group(1).strip() if category_match else \"Other\"\n",
        "\n",
        "    # Create a unique identifier for this entry\n",
        "    entry_id = f\"{normalize_string(title)}|{'|'.join(normalize_authors(authors))}\"\n",
        "\n",
        "    # Check if this entry has already been processed to avoid duplicates\n",
        "    if entry_id in processed_entries:\n",
        "        print(f\"Skipping duplicate entry: {title}\")\n",
        "        continue  # Skip adding the duplicate entry\n",
        "\n",
        "    # Add the entry to the set of processed entries\n",
        "    processed_entries.add(entry_id)\n",
        "\n",
        "    # Append to the appropriate category in the final JSON structure\n",
        "    category_map = {\n",
        "        \"Tables\": \"tables\",\n",
        "        \"Classification\": \"classification\",\n",
        "        \"Key Information Extraction\": \"keyInformationExtraction\",\n",
        "        \"Optical Character Recognition\": \"opticalCharacterRecognition\",\n",
        "        \"Datasets\": \"datasets\",\n",
        "        \"Document Layout Understanding\": \"layoutUnderstanding\"\n",
        "    }\n",
        "\n",
        "    # Retrieve the target category for the paper; if it's not recognized, assign it to \"others\"\n",
        "    target_category = category_map.get(category, \"others\")\n",
        "\n",
        "    # If the entry doesn't already exist in the target category, add it to the final JSON structure\n",
        "    if not entry_exists(final_json[target_category], title, authors):\n",
        "        final_json[target_category].append({\n",
        "            \"originalFileName\": file_name, # Include the original file name for reference\n",
        "            \"title\": title, # Add the extracted title\n",
        "            \"authors\": authors # Add the list of authors\n",
        "        })"
      ],
      "metadata": {
        "id": "rW1Bm9xLgY-P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save files in json format"
      ],
      "metadata": {
        "id": "rZ0aOWsRjH-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the JSON to a file\n",
        "output_file = '/content/test14.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(final_json, f, indent=4)\n",
        "\n",
        "# Optional: Download the file to your local system\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5BydpRALgTiI",
        "outputId": "c7616476-090e-4016-e9cb-5b3d3d2aaf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ce543a95-9bb8-4e80-a1e8-2be4b73977b9\", \"test13.json\", 48460)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}